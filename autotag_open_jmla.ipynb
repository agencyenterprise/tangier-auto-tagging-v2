{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [03:05<00:00, 92.72s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModel\n",
    "# import os\n",
    "\n",
    "# # Load the model\n",
    "# AutoModel.from_pretrained('meta-llama/Llama-2-7b-hf', \n",
    "#                                   token=os.getenv('HF_HUB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file wav_to_mel.py from https://github.com/taugastcn/SpectPrompt.git\n",
    "# !curl -O https://raw.githubusercontent.com/taugastcn/SpectPrompt/main/wav_to_mel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', trust_remote_code=True, token=os.getenv('HF_HUB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model = AutoModel.from_pretrained('UniMus/OpenJMLA', trust_remote_code=True,\n",
    "                                  token=os.getenv('HF_HUB_TOKEN'))\n",
    "                                #   use_auth_token=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAEForCausalLM(\n",
       "  (backbone): MAEViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (adaptive_padding): AdaptivePadding()\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (drop_after_pos): Dropout(p=0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0, inplace=False)\n",
       "          (out_drop): DropPath()\n",
       "        )\n",
       "        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (activate): GELU(approximate='none')\n",
       "          (layers): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (1): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (dropout_layer): DropPath()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (neck): LMDecoder(\n",
       "    (lm): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): mixEmbed(\n",
       "          (lm_embed): Embedding(32000, 4096)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "    (lm_embed): Embedding(32000, 4096)\n",
       "  )\n",
       "  (adapter): ModuleList(\n",
       "    (0-15): 16 x ModuleList(\n",
       "      (0): Adapter(\n",
       "        (adapter_norm_before): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (adapter_down): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        (non_linearity): SiLU()\n",
       "        (adapter_up): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (samplers): ModuleList(\n",
       "    (0-2): 3 x ModuleList(\n",
       "      (0): PerceiverAttentionLayer(\n",
       "        (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_latents): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = model.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample rate: 16k\n",
    "music_path = 'data/borhap/segment_0_10.wav'\n",
    "# 1. get logmelspectrogram\n",
    "from wav_to_mel import wav_to_mel\n",
    "lms = wav_to_mel(music_path)\n",
    "\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file transforms.py from https://github.com/taugastcn/SpectPrompt.git\n",
    "# !curl -O https://raw.githubusercontent.com/taugastcn/SpectPrompt/main/transforms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transforms import Normalize, SpecRandomCrop, SpecPadding, SpecRepeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAEForCausalLM(\n",
       "  (backbone): MAEViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (adaptive_padding): AdaptivePadding()\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (drop_after_pos): Dropout(p=0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0, inplace=False)\n",
       "          (out_drop): DropPath()\n",
       "        )\n",
       "        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (activate): GELU(approximate='none')\n",
       "          (layers): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (1): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (dropout_layer): DropPath()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (neck): LMDecoder(\n",
       "    (lm): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): mixEmbed(\n",
       "          (lm_embed): Embedding(32000, 4096)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "    (lm_embed): Embedding(32000, 4096)\n",
       "  )\n",
       "  (adapter): ModuleList(\n",
       "    (0-15): 16 x ModuleList(\n",
       "      (0): Adapter(\n",
       "        (adapter_norm_before): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (adapter_down): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        (non_linearity): SiLU()\n",
       "        (adapter_up): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (samplers): ModuleList(\n",
       "    (0-2): 3 x ModuleList(\n",
       "      (0): PerceiverAttentionLayer(\n",
       "        (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_latents): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = [ Normalize(-4.5, 4.5), SpecRandomCrop(target_len=2992), SpecPadding(target_len=2992), SpecRepeat() ]\n",
    "lms = lms.numpy()\n",
    "for trans in transforms:\n",
    "    lms = trans(lms)\n",
    "\n",
    "# 2. template of input\n",
    "input_dic = dict()\n",
    "input_dic['filenames'] = [music_path.split('/')[-1]]\n",
    "input_dic['ans_crds'] = [0]\n",
    "input_dic['audio_crds'] = [0]\n",
    "input_dic['attention_mask'] = torch.tensor([[1, 1, 1, 1, 1]]).to(device)\n",
    "input_dic['input_ids'] = torch.tensor([[1, 694, 5777, 683, 13]]).to(device)\n",
    "input_dic['spectrogram'] = torch.from_numpy(lms).unsqueeze(dim=0).to(device)\n",
    "# 3. generation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filenames', 'ans_crds', 'audio_crds', 'attention_mask', 'input_ids', 'spectrogram'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> This particular song falls under the pop genre and features vocals as the main element. The theme of the song revolves around summer and sunny weather, which is reflected in the overall mood of the music, which is happy and upbeat.\\n\\n 2012 remastered version of the song is a must-listen for anyone who enjoys lively and cheerful music.\\n\\n\\n\\n\\n\\n\\n. 2: 2: 998, a remastered version of a classic song. The song features a catchy melody and upbeat rhythm that is sure to get you moving.\\n\\n\\n\\n sing along to!\\n 2011111 by 10 more. Today, I guess?\\nthink by the Band http remix of this song. The song is in English and has a positive, uplifting vibe that is perfect for any occasion. The vocals are the main focus of the song, and they are delivered with precision and emotion, making it a joy to listen to. Overall, this is a great song to lift your spirits and put you in a good mood.\\n\\n\\non around the song \"Forean']\n"
     ]
    }
   ],
   "source": [
    "gen_ids = model.forward_test(input_dic)\n",
    "gen_text = model.neck.tokenizer.batch_decode(gen_ids.clip(0))\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> This particular song falls under the pop genre and features vocals as the main element. The theme of the song revolves around summer and sunny weather, which is reflected in the overall mood of the music, which is happy and upbeat.\\n\\n 2012 remastered version of the song is a must-listen for anyone who enjoys lively and cheerful music.\\n\\n\\n\\n\\n\\n\\n. 2: 2: 998, a remastered version of a classic song. The song features a catchy melody and upbeat rhythm that is sure to get you moving.\\n\\n\\n\\n sing along to!\\n 2011111 by 10 more. Today, I guess?\\nthink by the Band http remix of this song. The song is in English and has a positive, uplifting vibe that is perfect for any occasion. The vocals are the main focus of the song, and they are delivered with precision and emotion, making it a joy to listen to. Overall, this is a great song to lift your spirits and put you in a good mood.\\n\\n\\non around the song \"Forean']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Post-processing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Given that the training data may contain biases, the generated texts might need some straightforward post-processing to ensure accuracy.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# In future versions, we will enhance the quality of the data.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m gen_text \u001b[38;5;241m=\u001b[39m \u001b[43mgen_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      5\u001b[0m gen_text \u001b[38;5;241m=\u001b[39m gen_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in Chinese\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m gen_text \u001b[38;5;241m=\u001b[39m gen_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Chinese\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# 4. Post-processing\n",
    "# Given that the training data may contain biases, the generated texts might need some straightforward post-processing to ensure accuracy.\n",
    "# In future versions, we will enhance the quality of the data.\n",
    "gen_text = gen_text.split('<s>')[-1].split('\\n')[0].strip()\n",
    "gen_text = gen_text.replace(' in Chinese','')\n",
    "gen_text = gen_text.replace(' Chinese','')\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
