{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [03:05<00:00, 92.72s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(32000, 4096)\n",
       "  (layers): ModuleList(\n",
       "    (0-31): 32 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from transformers import AutoModel\n",
    "# import os\n",
    "\n",
    "# # Load the model\n",
    "# AutoModel.from_pretrained('meta-llama/Llama-2-7b-hf', \n",
    "#                                   token=os.getenv('HF_HUB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file wav_to_mel.py from https://github.com/taugastcn/SpectPrompt.git\n",
    "# !curl -O https://raw.githubusercontent.com/taugastcn/SpectPrompt/main/wav_to_mel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', trust_remote_code=True, token=os.getenv('HF_HUB_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.31s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model = AutoModel.from_pretrained('UniMus/OpenJMLA', trust_remote_code=True,\n",
    "                                  token=os.getenv('HF_HUB_TOKEN'))\n",
    "                                #   use_auth_token=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAEForCausalLM(\n",
       "  (backbone): MAEViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (adaptive_padding): AdaptivePadding()\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (drop_after_pos): Dropout(p=0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0, inplace=False)\n",
       "          (out_drop): DropPath()\n",
       "        )\n",
       "        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (activate): GELU(approximate='none')\n",
       "          (layers): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (1): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (dropout_layer): DropPath()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (neck): LMDecoder(\n",
       "    (lm): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): mixEmbed(\n",
       "          (lm_embed): Embedding(32000, 4096)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "    (lm_embed): Embedding(32000, 4096)\n",
       "  )\n",
       "  (adapter): ModuleList(\n",
       "    (0-15): 16 x ModuleList(\n",
       "      (0): Adapter(\n",
       "        (adapter_norm_before): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (adapter_down): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        (non_linearity): SiLU()\n",
       "        (adapter_up): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (samplers): ModuleList(\n",
       "    (0-2): 3 x ModuleList(\n",
       "      (0): PerceiverAttentionLayer(\n",
       "        (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_latents): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = model.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample rate: 16k\n",
    "music_path = 'data/borhap/segment_60_70.wav'\n",
    "# 1. get logmelspectrogram\n",
    "from wav_to_mel import wav_to_mel\n",
    "lms = wav_to_mel(music_path)\n",
    "\n",
    "import os\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the file transforms.py from https://github.com/taugastcn/SpectPrompt.git\n",
    "# !curl -O https://raw.githubusercontent.com/taugastcn/SpectPrompt/main/transforms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transforms import Normalize, SpecRandomCrop, SpecPadding, SpecRepeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAEForCausalLM(\n",
       "  (backbone): MAEViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (adaptive_padding): AdaptivePadding()\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (drop_after_pos): Dropout(p=0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (proj_drop): Dropout(p=0, inplace=False)\n",
       "          (out_drop): DropPath()\n",
       "        )\n",
       "        (ln2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (activate): GELU(approximate='none')\n",
       "          (layers): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "            (1): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (2): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (dropout_layer): DropPath()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (neck): LMDecoder(\n",
       "    (lm): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "    (lm_embed): Embedding(32000, 4096)\n",
       "  )\n",
       "  (adapter): ModuleList(\n",
       "    (0-15): 16 x ModuleList(\n",
       "      (0): Adapter(\n",
       "        (adapter_norm_before): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (adapter_down): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "        (non_linearity): SiLU()\n",
       "        (adapter_up): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (samplers): ModuleList(\n",
       "    (0-2): 3 x ModuleList(\n",
       "      (0): PerceiverAttentionLayer(\n",
       "        (norm_media): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm_latents): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "        (to_k): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_v): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = [ Normalize(-4.5, 4.5), SpecRandomCrop(target_len=2992), SpecPadding(target_len=2992), SpecRepeat() ]\n",
    "lms = lms.numpy()\n",
    "for trans in transforms:\n",
    "    lms = trans(lms)\n",
    "\n",
    "# 2. template of input\n",
    "input_dic = dict()\n",
    "input_dic['filenames'] = [music_path.split('/')[-1]]\n",
    "input_dic['ans_crds'] = [0]\n",
    "input_dic['audio_crds'] = [0]\n",
    "input_dic['attention_mask'] = torch.tensor([[1, 1, 1, 1, 1]]).to(device)\n",
    "input_dic['input_ids'] = torch.tensor([[1, 694, 5777, 683, 13]]).to(device)\n",
    "input_dic['spectrogram'] = torch.from_numpy(lms).unsqueeze(dim=0).to(device)\n",
    "# 3. generation\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filenames', 'ans_crds', 'audio_crds', 'attention_mask', 'input_ids', 'spectrogram'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s> This particular song falls under the genre of Hip Hop/Rap and features vocals in Chinese. The overall theme of the song is \"cool\" and the mood is \"dynamic\". The music itself is characterized by a strong beat and rhythmic flow, with a focus on the use of electronic instruments and synthesizers. The artist\\'s delivery is confident and energetic, adding to the overall vibe of the song.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nore to remember\\nii in Chinese culture and add a layer of depth to the song\\'s meaning.\\n\\n\\n\\n\\n\\n\\n\\n own to the music, one more. This is a remix, which is a unique take on the original song by the artist 胡彦辛. The song is a great example of how music can be used to convey a message and evoke emotions in the listener.\\n\\n\\n This is a must-listen for anyone who appreciates the artistry of hip hop and rap music.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\none to be, this is an excellent example of the use of electronic elements in hip hop music. The song has a strong beat and']\n"
     ]
    }
   ],
   "source": [
    "gen_ids = model.forward_test(input_dic)\n",
    "gen_text = model.neck.tokenizer.batch_decode(gen_ids.clip(0))\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text=gen_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This particular song falls under the genre of Hip Hop/Rap and features vocals. The overall theme of the song is \"cool\" and the mood is \"dynamic\". The music itself is characterized by a strong beat and rhythmic flow, with a focus on the use of electronic instruments and synthesizers. The artist's delivery is confident and energetic, adding to the overall vibe of the song.\n"
     ]
    }
   ],
   "source": [
    "# 4. Post-processing\n",
    "# Given that the training data may contain biases, the generated texts might need some straightforward post-processing to ensure accuracy.\n",
    "# In future versions, we will enhance the quality of the data.\n",
    "gen_text = gen_text.split('<s>')[-1].split('\\n')[0].strip()\n",
    "gen_text = gen_text.replace(' in Chinese','')\n",
    "gen_text = gen_text.replace(' Chinese','')\n",
    "print(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jmla",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
