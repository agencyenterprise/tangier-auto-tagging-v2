{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/speechbrain/speechbrain.git@develop\n",
    "# !pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tristan/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:speechbrain.utils.torch_audio_backend:SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom_interface.py: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "/Users/tristan/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/transformers/configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "WARNING:speechbrain.lobes.models.huggingface_transformers.huggingface:speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.\n",
      "WARNING:speechbrain.utils.torch_audio_backend:SpeechBrain could not find any working torchaudio backend. Audio files may fail to load. Follow this link for instructions and troubleshooting: https://speechbrain.readthedocs.io/en/latest/audioloading.html\n",
      "INFO:speechbrain.utils.fetching:Fetch wav2vec2.ckpt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch wav2vec2.ckpt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch model.ckpt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch model.ckpt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: wav2vec2, model, label_encoder\n",
      "/Users/tristan/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:199: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from speechbrain.inference.interfaces import foreign_class\n",
    "\n",
    "classifier = foreign_class(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", \n",
    "                           pymodule_file=\"custom_interface.py\", \n",
    "                           classname=\"CustomEncoderWav2vec2Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: _nxrYwT0SIo\n",
      "[youtube] _nxrYwT0SIo: Downloading webpage\n",
      "[youtube] _nxrYwT0SIo: Downloading ios player API JSON\n",
      "[youtube] _nxrYwT0SIo: Downloading mweb player API JSON\n",
      "[youtube] _nxrYwT0SIo: Downloading player 8579e400\n",
      "[youtube] _nxrYwT0SIo: Downloading m3u8 information\n",
      "[info] _nxrYwT0SIo: Downloading 1 format(s): 251\n",
      "[download] Bohemian Rhapsody [_nxrYwT0SIo].wav has already been downloaded\n",
      "[ExtractAudio] Destination: Bohemian Rhapsody [_nxrYwT0SIo].wav\n",
      "Deleting original file Bohemian Rhapsody [_nxrYwT0SIo].orig.wav (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "!yt-dlp -x --audio-format wav _nxrYwT0SIo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: segment_0_10.wav\n",
      "Exported: segment_10_20.wav\n",
      "Exported: segment_20_30.wav\n",
      "Exported: segment_30_40.wav\n",
      "Exported: segment_40_50.wav\n",
      "Exported: segment_50_60.wav\n",
      "Exported: segment_60_70.wav\n",
      "Exported: segment_70_80.wav\n",
      "Exported: segment_80_90.wav\n",
      "Exported: segment_90_100.wav\n",
      "Exported: segment_100_110.wav\n",
      "Exported: segment_110_120.wav\n",
      "Exported: segment_120_130.wav\n",
      "Exported: segment_130_140.wav\n",
      "Exported: segment_140_150.wav\n",
      "Exported: segment_150_160.wav\n",
      "Exported: segment_160_170.wav\n",
      "Exported: segment_170_180.wav\n",
      "Exported: segment_180_190.wav\n",
      "Exported: segment_190_200.wav\n",
      "Exported: segment_200_210.wav\n",
      "Exported: segment_210_220.wav\n",
      "Exported: segment_220_230.wav\n",
      "Exported: segment_230_240.wav\n",
      "Exported: segment_240_250.wav\n",
      "Exported: segment_250_260.wav\n",
      "Exported: segment_260_270.wav\n",
      "Exported: segment_270_280.wav\n",
      "Exported: segment_280_290.wav\n",
      "Exported: segment_290_300.wav\n",
      "Exported: segment_300_310.wav\n",
      "Exported: segment_310_320.wav\n",
      "Exported: segment_320_330.wav\n",
      "Exported: segment_330_340.wav\n",
      "Exported: segment_340_350.wav\n",
      "Exported: segment_350_360.wav\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "\n",
    "def split_wav(input_file, output_folder, segment_length_ms=10000):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_wav(input_file)\n",
    "    \n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get the total length of the audio in milliseconds\n",
    "    total_length_ms = len(audio)\n",
    "    \n",
    "    # Split the audio into 30-second segments\n",
    "    for i, start in enumerate(range(0, total_length_ms, segment_length_ms)):\n",
    "        end = start + segment_length_ms\n",
    "        segment = audio[start:end]\n",
    "        \n",
    "        # Generate output filename\n",
    "        output_filename = f\"segment_{start//1000}_{end//1000}.wav\"\n",
    "        output_path = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "        # Export the segment\n",
    "        segment.export(output_path, format=\"wav\")\n",
    "        print(f\"Exported: {output_filename}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"Bohemian Rhapsody [_nxrYwT0SIo].wav\"\n",
    "output_folder = \"data/borhap\"\n",
    "split_wav(input_file, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Songs that change genre](https://www.reddit.com/r/Music/comments/2xql0a/does_anybody_know_any_songs_that_dramatically/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://www.dropbox.com/s/u8qyvuyie2op286/spk1_snt1.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri spk1_snt1.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspeechbrain\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msb\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[43msb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspk1_snt1.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(source\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/speechbrain/dataio/dataio.py:278\u001b[0m, in \u001b[0;36mread_audio\u001b[0;34m(waveforms_obj)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"General audio loading, based on a custom notation.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03mExpected use case is in conjunction with Datasets\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03mTrue\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(waveforms_obj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 278\u001b[0m     audio, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveforms_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     path \u001b[38;5;241m=\u001b[39m waveforms_obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri spk1_snt1.wav and format None."
     ]
    }
   ],
   "source": [
    "import speechbrain as sb\n",
    "\n",
    "source = sb.dataio.dataio.read_audio('spk1_snt1.wav').squeeze()\n",
    "print(source.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure file exists \n",
    "assert os.path.isfile(\"data/borhap/segment_0_10.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri /Users/tristan/Documents/Code/AE/tangier-auto-tagging-v2/data/borhap/segment_0_10.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out_prob, score, index, text_lab \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/borhap/segment_0_10.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_lab)\n",
      "File \u001b[0;32m~/.cache/huggingface/hub/models--speechbrain--emotion-recognition-wav2vec2-IEMOCAP/snapshots/117a9c3dff08be81a3628eecf6a66b547ec1659b/custom_interface.py:144\u001b[0m, in \u001b[0;36mCustomEncoderWav2vec2Classifier.classify_file\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Classifies the given audiofile into the given set of labels.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    Arguments\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        (label encoder should be provided).\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Fake a batch:\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     batch \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/speechbrain/inference/interfaces.py:302\u001b[0m, in \u001b[0;36mPretrained.load_audio\u001b[0;34m(self, path, savedir)\u001b[0m\n\u001b[1;32m    295\u001b[0m source, fl \u001b[38;5;241m=\u001b[39m split_path(path)\n\u001b[1;32m    296\u001b[0m path \u001b[38;5;241m=\u001b[39m fetch(\n\u001b[1;32m    297\u001b[0m     fl,\n\u001b[1;32m    298\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[1;32m    299\u001b[0m     savedir\u001b[38;5;241m=\u001b[39msavedir,\n\u001b[1;32m    300\u001b[0m     local_strategy\u001b[38;5;241m=\u001b[39mLocalStrategy\u001b[38;5;241m.\u001b[39mNO_LINK,\n\u001b[1;32m    301\u001b[0m )\n\u001b[0;32m--> 302\u001b[0m signal, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_normalizer(signal, sr)\n",
      "File \u001b[0;32m~/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/Documents/Code/AE/tangier-auto-tagging-v2/.conda/lib/python3.11/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri /Users/tristan/Documents/Code/AE/tangier-auto-tagging-v2/data/borhap/segment_0_10.wav and format None."
     ]
    }
   ],
   "source": [
    "out_prob, score, index, text_lab = classifier.classify_file('data/borhap/segment_0_10.wav')\n",
    "print(text_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading [examples/10_soul or r&b.mp3]: 100%|██████████| 1.39M/1.39M [00:01<00:00, 1.36MB/s]\n",
      "Downloading [examples/12_Pop Vocal Ballad.mp3]: 100%|██████████| 2.60M/2.60M [00:01<00:00, 2.62MB/s]\n",
      "Downloading [examples/13_Adult Contemporary.mp3]: 100%|██████████| 2.74M/2.74M [00:01<00:00, 2.35MB/s]\n",
      "Downloading [examples/14_Teen Pop.mp3]: 100%|██████████| 1.16M/1.16M [00:00<00:00, 1.32MB/s]\n",
      "Downloading [examples/15_contemporary dance pop.mp3]: 100%|██████████| 1.64M/1.64M [00:01<00:00, 1.64MB/s]\n",
      "Downloading [examples/16_dance pop.mp3]: 100%|██████████| 2.81M/2.81M [00:00<00:00, 3.08MB/s]\n",
      "Downloading [examples/17_classic indie pop.mp3]: 100%|██████████| 1.91M/1.91M [00:01<00:00, 1.85MB/s]\n",
      "Downloading [examples/18_chamber,cabaret & art pop.mp3]: 100%|██████████| 2.26M/2.26M [00:00<00:00, 2.65MB/s]\n",
      "Downloading [examples/19_Adult Alternative Rock.mp3]: 100%|██████████| 2.19M/2.19M [00:00<00:00, 2.55MB/s]\n",
      "Downloading [examples/20_Uplifting Anthemic Rock.mp3]: 100%|██████████| 1.64M/1.64M [00:00<00:00, 1.92MB/s]\n",
      "Downloading [examples/21_Soft Rock.mp3]: 100%|██████████| 939k/939k [00:01<00:00, 944kB/s]\n",
      "Downloading [examples/22_Acoustic Pop.mp3]: 100%|██████████| 4.65M/4.65M [00:01<00:00, 4.20MB/s]\n",
      "Downloading [examples/3_symphony.mp3]: 100%|██████████| 3.90M/3.90M [00:01<00:00, 2.94MB/s]\n",
      "Downloading [examples/4_opera.mp3]: 100%|██████████| 2.23M/2.23M [00:01<00:00, 2.32MB/s]\n",
      "Downloading [examples/5_solo.mp3]: 100%|██████████| 1.78M/1.78M [00:01<00:00, 1.59MB/s]\n",
      "Downloading [examples/6_chamber.mp3]: 100%|██████████| 4.61M/4.61M [00:01<00:00, 4.07MB/s]\n",
      "Downloading [vgg19_bn_mel/acc.csv]: 100%|██████████| 1.67k/1.67k [00:00<00:00, 1.80kB/s]\n",
      "Downloading [convnext_base_cqt/acc.csv]: 100%|██████████| 1.68k/1.68k [00:00<00:00, 2.27kB/s]\n",
      "Downloading [vit_b_16_mel/acc.csv]: 100%|██████████| 1.84k/1.84k [00:00<00:00, 2.49kB/s]\n",
      "Downloading [inception_v3_mel/acc.csv]: 100%|██████████| 1.69k/1.69k [00:00<00:00, 2.31kB/s]\n",
      "Downloading [swin_b_mel/acc.csv]: 100%|██████████| 1.80k/1.80k [00:00<00:00, 2.14kB/s]\n",
      "Downloading [vit_b_16_cqt/acc.csv]: 100%|██████████| 1.66k/1.66k [00:01<00:00, 1.07kB/s]\n",
      "Downloading [vgg19_bn_cqt/acc.csv]: 100%|██████████| 1.68k/1.68k [00:00<00:00, 1.87kB/s]\n",
      "Downloading [vgg19_bn_cqt/acc.jpg]: 100%|██████████| 21.0k/21.0k [00:00<00:00, 23.0kB/s]\n",
      "Downloading [configuration.json]: 100%|██████████| 53.0/53.0 [00:00<00:00, 59.1B/s]\n",
      "Downloading [genres_results.png]: 100%|██████████| 112k/112k [00:00<00:00, 119kB/s]\n",
      "Downloading [vgg19_bn_cqt/loss.csv]: 100%|██████████| 627k/627k [00:01<00:00, 544kB/s]\n",
      "Downloading [convnext_base_cqt/loss.csv]: 100%|██████████| 607k/607k [00:00<00:00, 688kB/s]\n",
      "Downloading [inception_v3_mel/loss.csv]: 100%|██████████| 604k/604k [00:01<00:00, 619kB/s]\n",
      "Downloading [vit_b_16_cqt/loss.csv]: 100%|██████████| 623k/623k [00:01<00:00, 636kB/s]\n",
      "Downloading [vgg19_bn_mel/loss.csv]: 100%|██████████| 626k/626k [00:01<00:00, 601kB/s]\n",
      "Downloading [vit_b_16_mel/loss.csv]: 100%|██████████| 615k/615k [00:01<00:00, 574kB/s]\n",
      "Downloading [swin_b_mel/loss.csv]: 100%|██████████| 610k/610k [00:01<00:00, 583kB/s]\n",
      "Downloading [vgg19_bn_cqt/loss.jpg]: 100%|██████████| 20.4k/20.4k [00:01<00:00, 19.9kB/s]\n",
      "Downloading [vgg19_bn_mel/mat.csv]: 100%|██████████| 6.25k/6.25k [00:00<00:00, 7.29kB/s]\n",
      "Downloading [convnext_base_cqt/mat.csv]: 100%|██████████| 6.25k/6.25k [00:00<00:00, 7.20kB/s]\n",
      "Downloading [vgg19_bn_cqt/mat.csv]: 100%|██████████| 6.25k/6.25k [00:00<00:00, 6.85kB/s]\n",
      "Downloading [swin_b_mel/mat.csv]: 100%|██████████| 6.25k/6.25k [00:00<00:00, 6.56kB/s]\n",
      "Downloading [inception_v3_mel/mat.csv]: 100%|██████████| 6.25k/6.25k [00:01<00:00, 6.14kB/s]\n",
      "Downloading [vit_b_16_mel/mat.csv]: 100%|██████████| 6.25k/6.25k [00:00<00:00, 8.01kB/s]\n",
      "Downloading [vit_b_16_cqt/mat.csv]: 100%|██████████| 6.25k/6.25k [00:01<00:00, 6.19kB/s]\n",
      "Downloading [vgg19_bn_cqt/mat.jpg]: 100%|██████████| 49.9k/49.9k [00:01<00:00, 43.3kB/s]\n",
      "Downloading [plot.py]: 100%|██████████| 3.68k/3.68k [00:00<00:00, 3.94kB/s]\n",
      "Downloading [README.md]: 100%|██████████| 2.63k/2.63k [00:00<00:00, 3.53kB/s]\n",
      "Downloading [vgg19_bn_cqt/result.log]: 100%|██████████| 1.55k/1.55k [00:00<00:00, 1.63kB/s]\n",
      "Downloading [vgg19_bn_mel/result.log]: 100%|██████████| 1.55k/1.55k [00:00<00:00, 1.68kB/s]\n",
      "Downloading [convnext_base_cqt/result.log]: 100%|██████████| 1.58k/1.58k [00:00<00:00, 2.07kB/s]\n",
      "Downloading [vit_b_16_mel/result.log]: 100%|██████████| 1.55k/1.55k [00:00<00:00, 1.71kB/s]\n",
      "Downloading [swin_b_mel/result.log]: 100%|██████████| 1.54k/1.54k [00:00<00:00, 2.08kB/s]\n",
      "Downloading [inception_v3_mel/result.log]: 100%|██████████| 1.55k/1.55k [00:00<00:00, 1.79kB/s]\n",
      "Downloading [vit_b_16_cqt/result.log]: 100%|██████████| 1.55k/1.55k [00:01<00:00, 1.43kB/s]\n",
      "Downloading [convnext_base_cqt/save.pt]: 100%|██████████| 336M/336M [00:17<00:00, 20.5MB/s] \n",
      "Downloading [vit_b_16_cqt/save.pt]: 100%|██████████| 328M/328M [00:07<00:00, 47.5MB/s] \n",
      "Downloading [vgg19_bn_mel/save.pt]: 100%|██████████| 465M/465M [00:09<00:00, 51.8MB/s] \n",
      "Downloading [vit_b_16_mel/save.pt]: 100%|██████████| 328M/328M [00:08<00:00, 41.8MB/s] \n",
      "Downloading [vgg19_bn_cqt/save.pt]: 100%|██████████| 465M/465M [00:18<00:00, 25.8MB/s] \n",
      "Downloading [swin_b_mel/save.pt]: 100%|██████████| 333M/333M [00:13<00:00, 26.8MB/s] \n",
      "Downloading [inception_v3_mel/save.pt]: 100%|██████████| 101M/101M [00:04<00:00, 23.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('ccmusic-database/music_genre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "# Load the model\n",
    "model_id = 'ccmusic-database/music_genre'\n",
    "genre_classification = pipeline(Tasks.audio_classification, model=model_id)\n",
    "\n",
    "# Prepare your input\n",
    "audio_path = 'data/borhap/segment_0_10.wav'  # Replace with your audio file path\n",
    "\n",
    "# Run inference\n",
    "result = genre_classification(audio_path)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
